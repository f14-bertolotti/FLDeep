\section{Introduction}\label{sects:introduction}

One of the most popular deep learning (DL) architecture for natural language processing (NLP) is the Transformer architecture. The Transformer was popularized in the famous paper "Attention is all you need" \cite{Vaswani17}. From the original proposal, many architectural improvements have been proposed, e.g. \cite{Devlin18, Liu19, Yang19} and many other. Recently, the Transformer is being applied also to task for which wasn't originally designed. One notable example is images processing. In particular, \cite{Dosovitskiy20} adapt the Transformer architecture to process images. This represents one of the firsts instances of convolution free architecture to achieve comparable results with the state-of-the-art (SOTA). The same idea is further explored in \cite{Wang21} using a pyramidal-like Transformer architecture. This architecture achieves further improvements surpassing results obtained by ResNets architectures \cite{He16}. While the Transformer architecture is gaining further traction in image processing tasks the potentialities reamain mostly unexplored. With this project, we aim to apply and adapt the Transformer for the task of "Facial Landmark Detection". While, most of the previous work aim to a convolution-free architectures, we aim to combine the ResNets and the Transformers hoping to gain further improvements.

