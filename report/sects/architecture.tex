\section{Combining ResNets with Transformers}\label{sect:architecture}

In this section, we are going to discuss the chosen architecture. Firstly, we consider a pre-trained ResNet on ImageNet. In particular, ResNet101. We proceed by gaining access to different layers of the ResNet:

\begin{lstlisting}[language=Python]
self.resnet  = torchvision.models.resnet101(pretrained=True, progress=True)
self.resnet3 = torch.nn.Sequential(*list(self.resnet.children())[6:8])
self.resnet2 = torch.nn.Sequential(*list(self.resnet.children())[5:6])
self.resnet1 = torch.nn.Sequential(*list(self.resnet.children())[3:5])
self.resnet0 = torch.nn.Sequential(*list(self.resnet.children())[0:3])
...
res0 = self.resnet0(imgs)
res1 = self.resnet1(res0)
res2 = self.resnet2(res1)
res3 = self.resnet3(res2)
\end{lstlisting}

In the above listing, we split ResNet101 into four steps. These steps give us access to low-level and high-level features (\texttt{res0,res1,res2,res3}). These features will be fed to our Transformer Encoder. In particular, the sizes of these features are: $\text{imgs}\in\mathbb{R}^{b\times 3\times 224\times224}$, $\text{res}_0\in\mathbb{R}^{b\times64\times 112 \times 112}$, $\text{res}_1\in\mathbb{R}^{b\times256\times 56\times 56}$, $\text{res}_2\in\mathbb{R}^{b\times512\times 28\times 28}$, $\text{res}_3\in\mathbb{R}^{b\times2048 \times 7\times 7}$. Where $b$ represents the batch size. Next, we interpret these features as images. Thus, we can subdivide these images into patches.

\begin{lstlisting}[language=Python]
from einops import rearrange as r
res0 = r(res0,"b c (h1 h2) (w1 w2) -> b (c h1 w1) (h2 w2)",h1=2,w1=2)
res1 = r(res1,"b c (h1 h2) (w1 w2) -> b (c h1 w1) (h2 w2)",h1=2,w1=2)
res2 = r(res2,"b c (h1 h2) (w1 w2) -> b (c h1 w1) (h2 w2)",h1=2,w1=2)
res3 = r(res3,"b c h w -> b (h w) c")
\end{lstlisting}

Now, \texttt{res0,res1,res2}, and \texttt{res3} contains more channels with linearized patches. Once again, it is worth looking at the shapes of these tensors. $\text{res}_0\in\mathbb{R}^{b\times1024\times784}$, $\text{res}_1\in\mathbb{R}^{b\times1024\times784}$, $\text{res}_2\in\mathbb{R}^{b\times2048\times196}$, $\text{res}_3\in\mathbb{R}^{b\times2048\times49}$. Now, it should not be too hard to interpret these linearized patches as tokens (much like in NLP) and feed them to a Transformer. However, there are two considerations to be made. Firstly, all patch sizes should be the same. To achieve this, we can employ a feed-forward network. Secondly, we have $1024+1024+2048+2048=6144$ tokens which require too much memory. Thus, to reduce the memory footprint, we average several tokens together.

\begin{lstlisting}[language=Python]
from einops import reduce as rd
res0 = rd(ffB0(dropout(gelu(ffA0(res0)))),"b e (c1 c2)->b c1 e","max",c2=16)
res1 = rd(ffB1(dropout(gelu(ffA1(res1)))),"b e (c1 c2)->b c1 e","max",c2=8)
res2 = rd(ffB2(dropout(gelu(ffA2(res2)))),"b e (c1 c2)->b c1 e","max",c2=4)
res3 = rd(ffB3(dropout(gelu(ffA3(res2)))),"b e (c1 c2)->b c1 e","max",c2=1)
\end{lstlisting}

With the previous snippet, we solved the discussed issues. Now, we have a reasonable amount of tokens (441) each one of them having the same size (512). We can concatenate these tokens together and proceed to add the positional embeddings.

\begin{lstlisting}[language=Python]
self.positions = torch.nn.Embedding(441,512).weight.unsqueeze(0)
...
srcs = torch.cat([res0,res1,res2,res3],1) 
srcs += self.positions.repeat(batch_size,1,1)
\end{lstlisting}

At this point, we have prepared the input for the transformer. We feed the prepared patches to the Transformer. Next, we can downscale the first $68$ tokens to only $two$ features representing the landmark position on the face.

\begin{lstlisting}[language=Python]
from torch.nn import TransformerEncoder
from torch.nn import TransformerEncoderLayer
self.encoder = TransformerEncoder(
                    TransformerEncoderLayer(512,
                        nhead=8,
                        dim_feedforward=2048,
                        activation="gelu",
                        dropout=.1),
                    num_layers=4)
self.downscale = torch.nn.Linear(512,2)
...
mems = self.encoder(srcs.transpose(0,1)).transpose(0,1)
ldmk = self.downscale(mems[:,:68])
\end{lstlisting}

With this last piece, we have practically defined the entire architecture. The only thing that remained to discuss is the training procedure. We used an \textit{l1 loss} with \textit{Adam} optimizer, \textit{learning rate} set to $1e^{-4}$, and \textit{batch size} of $32$.

Since the proposed dataset (300W) is quite small, we introduce three augmentation techniques. 
\begin{enumerate}
    \item random rotations.
    \item random crop.
    \item random jitter.
\end{enumerate}

