\section{Conclusion \& Future Works}\label{sects:future}

In the last section, we have shown that a transformer can be used to achieve results close to the state-of-the-art for the task of Facial Landmark detection. While the results obtained are encouraging there are still imperfections. Consider figures~\ref{figs:preds}, the predicted fiducial points are quite close to expected ones. However, it is evident that the model is biased towards symmetric mouth expressions. Moreover, often predicted points do not align perfectively with the represeted feature. Of course, many other things that could be done or improved to achieve better results. First of all, introducing more augmentation approaches could be beneficial. Secondly, deeper resnets and deeper transformer encoders could boost the performance. It could be worth trying other strategies to extract information from a resnet to be fed to a transformer.
